{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f27a88b",
   "metadata": {},
   "source": [
    "## Steps\n",
    " - 1. Create Vocab\n",
    " - 2. Encode words\n",
    " - 3. Seperate into input/output, put into <b>LongTensor</b>\n",
    " - 4. Model class\n",
    " - 5. Training params\n",
    " - 6. Train\n",
    " - 7. Test\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0a95ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "268bc418",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f3b7cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['bob likes sheep', 'alice is fast', 'cs685 is fun', 'you love lamp', \\\n",
    "             \"i like dog\", \"i love coffee\", \"i hate milk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5f110ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bob': 0, 'likes': 1, 'sheep': 2, 'alice': 3, 'is': 4, 'fast': 5, 'cs685': 6, 'fun': 7, 'you': 8, 'love': 9, 'lamp': 10, 'i': 11, 'like': 12, 'dog': 13, 'coffee': 14, 'hate': 15, 'milk': 16}\n",
      "[[0, 1, 2], [3, 4, 5], [6, 4, 7], [8, 9, 10], [11, 12, 13], [11, 9, 14], [11, 15, 16]]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "inv_vocab = {}\n",
    "sent_ids = []\n",
    "\n",
    "for sent in sentences:\n",
    "    word_ids = []\n",
    "    for word in sent.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "            inv_vocab[len(vocab)-1] = word\n",
    "            \n",
    "        word_ids.append(vocab[word])        \n",
    "            \n",
    "    sent_ids.append(word_ids)    \n",
    "    \n",
    "print(vocab)\n",
    "print(sent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34cc49da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1],\n",
       "         [ 3,  4],\n",
       "         [ 6,  4],\n",
       "         [ 8,  9],\n",
       "         [11, 12],\n",
       "         [11,  9],\n",
       "         [11, 15]]),\n",
       " tensor([ 2,  5,  7, 10, 13, 14, 16]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([sent[:2] for sent in sent_ids])\n",
    "label_ids = torch.LongTensor([sent[-1] for sent in sent_ids])\n",
    "input_ids, label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d7d79",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2cee54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLM(nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_dim: int, vocab_dim: int, seq_len: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_dim, embedding_dim=embed_dim)\n",
    "        self.fc1 = nn.Linear(seq_len*embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_dim)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         print(\"Input shape: \", x.shape)\n",
    "        # shape (batch, seq_len) -> (batch, seq_len, embed_dim)\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "#         print(\"Embeddings shape: \", x.shape)\n",
    "        # concatenate all token embeddings????\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "#         print(\"Before 1 layer shape: \", x.shape)\n",
    "        # shape (batch, seq_len*embed_dim) -> (batch, hidden_dim)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        \n",
    "#         print(\"Before 2 layer shape: \", x.shape)\n",
    "        # shape (batch, hidden_dim) - > (batch, vocab_dim)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "#         print(\"Output shape: \", x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c9e67",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a6bf180e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 2.806\n",
      "Epoch 1 loss = 1.911\n",
      "Epoch 2 loss = 1.331\n",
      "Epoch 3 loss = 0.877\n",
      "Epoch 4 loss = 0.575\n",
      "Epoch 5 loss = 0.389\n",
      "Epoch 6 loss = 0.265\n",
      "Epoch 7 loss = 0.186\n",
      "Epoch 8 loss = 0.134\n",
      "Epoch 9 loss = 0.098\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "\n",
    "model = NLM(embed_dim=10, hidden_dim=8, vocab_dim=len(vocab), seq_len=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1) # SGD\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #shuffle\n",
    "    perms = torch.randperm(input_ids.shape[0])\n",
    "    batch_inputs = input_ids[perms]\n",
    "    batch_labels = label_ids[perms]\n",
    "    \n",
    "    # predict\n",
    "    output_ids = model(batch_inputs)\n",
    "    \n",
    "    # loss\n",
    "    loss = loss_fn(output_ids, batch_labels)\n",
    "\n",
    "    # backward    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f\"Epoch {epoch} loss = {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b129e45",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8ed1030f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i hate milk'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 6\n",
    "sentences[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "57aba854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = input_ids[num].view(1, -1)\n",
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3ecd84f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3127, -1.3140, -4.2429, -1.4105, -1.1098,  0.1548, -1.4311, -2.2945,\n",
       "         -1.4624, -1.3655, -0.0172, -1.1250, -1.5852,  2.4191,  2.4025, -1.3542,\n",
       "          6.3865]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = model(test_input)\n",
    "\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9f1c2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1a002296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'milk'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.argmax(softmax(test_output)).item()\n",
    "inv_vocab[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4b18ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bob likes sheep -> sheep\n",
      "alice is fast -> fast\n",
      "cs685 is fun -> fun\n",
      "you love lamp -> lamp\n",
      "i like dog -> dog\n",
      "i love coffee -> coffee\n",
      "i hate milk -> milk\n"
     ]
    }
   ],
   "source": [
    "for num, sent in enumerate(sentences):\n",
    "    test_input = input_ids[num].view(1, -1)\n",
    "    test_output = model(test_input)\n",
    "    idx = torch.argmax(softmax(test_output)).item()\n",
    "    \n",
    "    \n",
    "    print(sent, '->', inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d86093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea82c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exps",
   "language": "python",
   "name": "exps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
